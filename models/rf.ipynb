{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps: \n",
    "1. Load the merged_df dataset. This has 111 rows. \n",
    "2. Split it into train and test. Test should be 0.3, this leads to better recall. \n",
    "3. Apply SMOTE on the train set. This creates X_train_res (res stands for resampled) and y_train_res. \n",
    "4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, auc, precision_recall_curve, confusion_matrix\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merged df is the dataframe I created with all the extracted features from TS fresh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sum_12</th>\n",
       "      <th>sum_4567</th>\n",
       "      <th>sum_456</th>\n",
       "      <th>VVR_group</th>\n",
       "      <th>Condition</th>\n",
       "      <th>VVR_1</th>\n",
       "      <th>VVR_2</th>\n",
       "      <th>AU01_r__sum_values</th>\n",
       "      <th>AU01_r__variance</th>\n",
       "      <th>...</th>\n",
       "      <th>AU26_r__minimum</th>\n",
       "      <th>AU26_r__mean</th>\n",
       "      <th>AU26_r__mean_abs_change</th>\n",
       "      <th>AU45_r__sum_values</th>\n",
       "      <th>AU45_r__variance</th>\n",
       "      <th>AU45_r__standard_deviation</th>\n",
       "      <th>AU45_r__maximum</th>\n",
       "      <th>AU45_r__minimum</th>\n",
       "      <th>AU45_r__mean</th>\n",
       "      <th>AU45_r__mean_abs_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>24.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4982.48</td>\n",
       "      <td>0.425041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.633284</td>\n",
       "      <td>0.076328</td>\n",
       "      <td>9231.74</td>\n",
       "      <td>0.825039</td>\n",
       "      <td>0.908316</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.627753</td>\n",
       "      <td>0.133624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>23.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9390.23</td>\n",
       "      <td>0.448366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.484701</td>\n",
       "      <td>0.125851</td>\n",
       "      <td>11887.00</td>\n",
       "      <td>0.634554</td>\n",
       "      <td>0.796589</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.436942</td>\n",
       "      <td>0.098134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>28.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6954.35</td>\n",
       "      <td>0.599805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.862301</td>\n",
       "      <td>0.101969</td>\n",
       "      <td>9020.78</td>\n",
       "      <td>0.750701</td>\n",
       "      <td>0.866430</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.550652</td>\n",
       "      <td>0.085720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26</td>\n",
       "      <td>30.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9707.43</td>\n",
       "      <td>0.873280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.552359</td>\n",
       "      <td>0.069582</td>\n",
       "      <td>6585.31</td>\n",
       "      <td>0.609348</td>\n",
       "      <td>0.780607</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.371673</td>\n",
       "      <td>0.056287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>22.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>21049.90</td>\n",
       "      <td>1.475421</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.92</td>\n",
       "      <td>0.142027</td>\n",
       "      <td>0.386527</td>\n",
       "      <td>23027.73</td>\n",
       "      <td>1.160635</td>\n",
       "      <td>1.077328</td>\n",
       "      <td>5.04</td>\n",
       "      <td>-4.29</td>\n",
       "      <td>1.094318</td>\n",
       "      <td>0.231853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  sum_12  sum_4567  sum_456  VVR_group  Condition  VVR_1  VVR_2  \\\n",
       "0  23    24.0      37.0     27.0          0          2   13.0   11.0   \n",
       "1  24    23.0      37.0     28.0          0          2   12.0   11.0   \n",
       "2  25    28.0      44.0     33.0          1          2   16.0   12.0   \n",
       "3  26    30.0      37.0     29.0          0          1   15.0   15.0   \n",
       "4  27    22.0      39.0     31.0          1          2   11.0   11.0   \n",
       "\n",
       "   AU01_r__sum_values  AU01_r__variance  ...  AU26_r__minimum  AU26_r__mean  \\\n",
       "0             4982.48          0.425041  ...             0.00      0.633284   \n",
       "1             9390.23          0.448366  ...             0.00      1.484701   \n",
       "2             6954.35          0.599805  ...             0.00      0.862301   \n",
       "3             9707.43          0.873280  ...             0.00      0.552359   \n",
       "4            21049.90          1.475421  ...            -3.92      0.142027   \n",
       "\n",
       "   AU26_r__mean_abs_change  AU45_r__sum_values  AU45_r__variance  \\\n",
       "0                 0.076328             9231.74          0.825039   \n",
       "1                 0.125851            11887.00          0.634554   \n",
       "2                 0.101969             9020.78          0.750701   \n",
       "3                 0.069582             6585.31          0.609348   \n",
       "4                 0.386527            23027.73          1.160635   \n",
       "\n",
       "   AU45_r__standard_deviation  AU45_r__maximum  AU45_r__minimum  AU45_r__mean  \\\n",
       "0                    0.908316             4.91             0.00      0.627753   \n",
       "1                    0.796589             5.00             0.00      0.436942   \n",
       "2                    0.866430             4.04             0.00      0.550652   \n",
       "3                    0.780607             4.90             0.00      0.371673   \n",
       "4                    1.077328             5.04            -4.29      1.094318   \n",
       "\n",
       "   AU45_r__mean_abs_change  \n",
       "0                 0.133624  \n",
       "1                 0.098134  \n",
       "2                 0.085720  \n",
       "3                 0.056287  \n",
       "4                 0.231853  \n",
       "\n",
       "[5 rows x 127 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df = pd.read_csv('/Users/dionnespaltman/Desktop/V3/merged_df.csv', sep=',')\n",
    "\n",
    "merged_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "merged_df.drop('Unnamed: 0.1', axis=1, inplace=True)\n",
    "\n",
    "display(merged_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to know how many people are in each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in VVR_group = 1: 26\n",
      "Number of instances in VVR_group = 0: 85\n"
     ]
    }
   ],
   "source": [
    "# Count the number of instances of people in VVR_group = 1 and VVR_group = 0\n",
    "count_vvr_group = merged_df['VVR_group'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Number of instances in VVR_group = 1:\", count_vvr_group[1])\n",
    "print(\"Number of instances in VVR_group = 0:\", count_vvr_group[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [ 'ID', 'sum_12', 'sum_4567', 'sum_456', 'VVR_group', 'Condition'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df.drop(columns_to_drop, axis=1)\n",
    "y = merged_df['VVR_group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the columns I use to predict, so all my features. I need these as a list to establish my featurizer. \n",
    "I have 119 features from TS fresh and then I added the two VVR measurements from stage 1 and 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/dionnespaltman/Desktop/V3/columns_au_12.json', 'r') as f:\n",
    "    columns_au_12 = json.load(f)\n",
    "\n",
    "print(len(columns_au_12))\n",
    "# print(columns_au_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll split the data into a train and test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train set has 88 participants, the test set has 23 participants. \n",
    "\n",
    "I have made the test_size 30% instead of 20%. \n",
    "Then I tested test_size 40% and that made the recall slightly higher (but not by much). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 127)\n",
      "(45, 127)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(merged_df, test_size=0.4, random_state=123, stratify=merged_df['VVR_group'])\n",
    "# train, val = train_test_split(train, stratify=train['VVR_group'], random_state=123)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the test set is very small with only 8 people in the high VVR condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 34, 1: 11})\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [ 'ID', 'sum_12', 'sum_4567', 'sum_456', 'VVR_group', 'Condition'] \n",
    "\n",
    "X_test = test.drop(columns_to_drop, axis=1)\n",
    "y_test = test['VVR_group']\n",
    "\n",
    "# Print original class distribution\n",
    "print('Original dataset shape %s' % Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 51, 1: 15})\n",
      "Resampled dataset shape Counter({1: 51, 0: 51})\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [ 'ID', 'sum_12', 'sum_4567', 'sum_456', 'VVR_group', 'Condition'] \n",
    "\n",
    "X_train = train.drop(columns_to_drop, axis=1)\n",
    "y_train = train['VVR_group']\n",
    "\n",
    "# Print original class distribution\n",
    "print('Original dataset shape %s' % Counter(y_train))\n",
    "\n",
    "# Apply SMOTE to the training data with sampling strategy set to 'auto' (default)\n",
    "sm = SMOTE(sampling_strategy='not majority', random_state=42, k_neighbors=5)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print resampled class distribution\n",
    "print('Resampled dataset shape %s' % Counter(y_train_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will add class weights to my models, because of my inbalanced data set. \n",
    "https://medium.com/@ravi.abhinav4/improving-class-imbalance-with-class-weights-in-machine-learning-af072fdd4aa4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 0.6470588235294118, 1: 2.2}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_class_weights(y):\n",
    "    unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "    total_samples = len(y)\n",
    "    class_weights = {}\n",
    "\n",
    "    for class_label, class_count in zip(unique_classes, class_counts):\n",
    "        class_weight = total_samples / (2.0 * class_count)\n",
    "        class_weights[class_label] = class_weight\n",
    "\n",
    "    return class_weights\n",
    "\n",
    "# Assuming 'y' contains the class labels (0s and 1s) for the binary classification problem\n",
    "class_weights = calculate_class_weights(y_train)\n",
    "print(\"Class weights:\", class_weights)\n",
    "\n",
    "# this is how you would implement it \n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = ColumnTransformer(transformers=[(\"numeric\", StandardScaler(), columns_au_12)], remainder='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strange enough the recall is much higher (0.27 instead of 0.18) when I don't use class weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({1: 51, 0: 51})\n",
      "Accuracy on Validation Data: 0.7777777777777778\n",
      "AUC-PR on Validation Data: 0.5252525252525253\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.94      0.86        34\n",
      "           1       0.60      0.27      0.38        11\n",
      "\n",
      "    accuracy                           0.78        45\n",
      "   macro avg       0.70      0.61      0.62        45\n",
      "weighted avg       0.75      0.78      0.75        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "\n",
    "# model = make_pipeline(featurizer, RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=0, class_weight=class_weights))\n",
    "model = make_pipeline(featurizer, RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=0))\n",
    "\n",
    "model.fit(X_train_res, y_train_res)\n",
    "\n",
    "print('Original dataset shape %s' % Counter(y_train_res))\n",
    "\n",
    "pred = model.predict(test.drop('VVR_group', axis=1))\n",
    "accuracy = accuracy_score(test['VVR_group'].values, pred)\n",
    "report = classification_report(test['VVR_group'].values, pred)\n",
    "cm = confusion_matrix(test['VVR_group'].values, pred)\n",
    "precision, recall, _ = precision_recall_curve(test['VVR_group'].values, pred)\n",
    "auc_pr = auc(recall, precision)\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy on Validation Data: {accuracy}\")\n",
    "print(f\"AUC-PR on Validation Data: {auc_pr}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive feature elimination to optimize the scores \n",
    "https://machinelearningmastery.com/rfe-feature-selection-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111, 121)\n",
      "(111,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is down below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.533 (0.306)\n"
     ]
    }
   ],
   "source": [
    "# evaluate RFE for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create pipeline\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=10)\n",
    "model = DecisionTreeClassifier()\n",
    "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='recall', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Recall: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.458 (0.302)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dionnespaltman/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# evaluate RFE for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create pipeline\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
    "model = DecisionTreeClassifier()\n",
    "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='precision', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Precision: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.736 (0.121)\n"
     ]
    }
   ],
   "source": [
    "# evaluate RFE for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create pipeline\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
    "model = DecisionTreeClassifier()\n",
    "pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "# evaluate model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall per class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\\n             13,  14,  15,  16,  17,  18,  19,  22,  24,  25,  26,  27,  28,\\n             29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\\n             42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  55,\\n             56,  57,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\\n             71,  73,  74,  75,  76,  77,  79,  80,  81,  82,  83,  84,  85,\\n             86,  87,  88,  89,  90,  91,  93,  94,  96,  97,  99, 100, 101,\\n            103, 104, 105, 106, 107, 108, 109, 110],\\n           dtype='int64')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[227], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m cv \u001b[38;5;241m=\u001b[39m RepeatedStratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, n_repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y):\n\u001b[0;32m---> 15\u001b[0m     X_train, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m]\u001b[49m, X[test_index]\n\u001b[1;32m     16\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m y[train_index], y[test_index]\n\u001b[1;32m     18\u001b[0m     pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:5796\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5794\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5798\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   5799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5800\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:5856\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5855\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 5856\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5858\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   5859\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\\n             13,  14,  15,  16,  17,  18,  19,  22,  24,  25,  26,  27,  28,\\n             29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\\n             42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  55,\\n             56,  57,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\\n             71,  73,  74,  75,  76,  77,  79,  80,  81,  82,  83,  84,  85,\\n             86,  87,  88,  89,  90,  91,  93,  94,  96,  97,  99, 100, 101,\\n            103, 104, 105, 106, 107, 108, 109, 110],\\n           dtype='int64')] are in the [columns]\""
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
